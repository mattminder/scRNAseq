{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "from neural_net import NeuralNet\n",
    "from NN_utils import read_labels, read_data\n",
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 500)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "#x_name = 'data/train_data_GS.csv' \n",
    "y_name = 'data/response.csv'\n",
    "\n",
    "#X = read_data(x_name)\n",
    "y = read_labels(y_name)\n",
    "\n",
    "X = np.load('data/train_500pcs.npy')\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (6000, 500)\n",
      "Train labels shape:  (6000,)\n",
      "Validation data shape:  (0, 500)\n",
      "Validation labels shape:  (0,)\n"
     ]
    }
   ],
   "source": [
    "# Split into train and validation\n",
    "# such that nb of class in train and validation are equal\n",
    "#percent = 0.15 # If we want to validate on a part of the train data\n",
    "percent = 0\n",
    "idx_true = np.random.permutation([i for i in range(len(y)) if y[i]==1]) # permute so that choice is random when taking the first n elements\n",
    "idx_false = np.random.permutation([i for i in range(len(y)) if y[i]==0])\n",
    "idx_val = np.append(idx_true[:int(len(idx_true)*percent)],idx_false[:int(len(idx_false)*percent)])\n",
    "idx_train = np.append(idx_true[int(len(idx_true)*percent):],idx_false[int(len(idx_false)*percent):])\n",
    "\n",
    "X_train = X[idx_train,:]\n",
    "y_train = y[idx_train]\n",
    "X_val = X[idx_val,:]\n",
    "y_val = y[idx_val]\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train = torch.from_numpy(X_train).type(torch.FloatTensor), torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "X_val, y_val = torch.from_numpy(X_val).type(torch.FloatTensor), torch.from_numpy(y_val).type(torch.LongTensor)\n",
    "\n",
    "traindataset = utils.TensorDataset(X_train, y_train)\n",
    "trainloader = utils.DataLoader(traindataset, batch_size=100, shuffle=True)\n",
    "if len(y_val) > 0:\n",
    "    val_batch_size = 50\n",
    "    valdataset = utils.TensorDataset(X_val, y_val)\n",
    "    valloader = utils.DataLoader(valdataset, batch_size=val_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choice of hyperparameters\n",
    "net = NeuralNet(n_input_channels=X_train.shape[1])\n",
    "lr = 6.6*10**(-2)\n",
    "reg = 2.2*10**(-10)\n",
    "momentum = 0.95\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training and validation\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 10\n",
    "for e in range(epochs):\n",
    "    lr*0.01**(e/epochs) # Learning rate decay, starting from best lr from val_hyperparameters\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr = lr, weight_decay = reg, momentum = momentum)\n",
    "    start = time.time()\n",
    "    for data, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        #transofrm inputs and outputs into Variable \n",
    "        inputs, targets = Variable(data), Variable(labels)\n",
    "\n",
    "        #set gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        out = net.forward(inputs)\n",
    "\n",
    "        loss = criterion(out, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.data.item()\n",
    "\n",
    "        if steps % print_every == 0 and len(y_val) > 0:\n",
    "            stop = time.time()\n",
    "            # Validation accuracy\n",
    "            accuracy = 0\n",
    "            for ii, (data, labels) in enumerate(valloader):\n",
    "                out = net.predict(Variable(data))\n",
    "                _, prediction = torch.max(out, 1)\n",
    "                #pred_y = prediction.data.cpu().numpy().squeeze() #if run on cluster\n",
    "                pred_y = prediction.data.numpy().squeeze()\n",
    "                target_y = (labels.numpy()).data\n",
    "                accuracy += sum(pred_y == target_y)/val_batch_size\n",
    "\n",
    "            print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}..\".format(running_loss/print_every),\n",
    "                  \"Test accuracy: {:.4f}..\".format(accuracy/(ii+1)),\n",
    "                  \"{:.4f} s/batch..\".format((stop - start)/print_every),\n",
    "                  \"Learning rate: {:.3e}\".format(lr)\n",
    "                 )\n",
    "            running_loss = 0\n",
    "            start = time.time()\n",
    "\n",
    "# Save end state            \n",
    "torch.save(net.state_dict(), 'neuralNet.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9523276e-01, 4.7671841e-03],\n",
       "       [9.8600686e-01, 1.3993122e-02],\n",
       "       [1.3504068e-02, 9.8649591e-01],\n",
       "       ...,\n",
       "       [9.8461884e-01, 1.5381089e-02],\n",
       "       [5.5209854e-09, 1.0000000e+00],\n",
       "       [4.7892823e-05, 9.9995208e-01]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Direct way to run prediction and save as np and csv\n",
    "from prediction import prediction\n",
    "prediction('data/herring_500pcs.npy', result_file_np='results/neural_net_herring_500pcs.npy', result_file_csv='results/neural_net_herring_500pcs.csv')\n",
    "prediction('data/joost_500pcs.npy', result_file_np='results/neural_net_joost_500pcs.npy', result_file_csv='results/neural_net_joost_500pcs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
